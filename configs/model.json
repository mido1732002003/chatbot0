{
  "vocab_size": 8192,
  "d_model": 640,
  "n_layers": 8,
  "n_heads": 8,
  "d_ff": 2560,
  "max_seq_len": 256,
  "dropout": 0.1,
  "tie_weights": true,
  "model_type": "transformer_lm",
  "description": "Default ~50M parameter model configuration"
}